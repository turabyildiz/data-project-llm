{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')\n",
    "\n",
    "# Zip the saved model directory\n",
    "\n",
    "zip -r model.zip ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zipped model\n",
    "\n",
    "from google.colab import files\n",
    "files.download('model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, pipeline\n",
    "import os\n",
    "\n",
    "# Local path to model directory \n",
    "model_path = r'/content/model'\n",
    "\n",
    "# Verify that the path exists and contains necessary files\n",
    "\n",
    "required_files = [\n",
    "    \"config.json\",\n",
    "    \"model.safetensors\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"vocab.txt\",\n",
    "    \"special_tokens_map.json\"\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if not os.path.exists(os.path.join(model_path, file)):\n",
    "        raise FileNotFoundError(f\"Required file {file} not found in {model_path}\")\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create a pipeline with the loaded tokenizer and model\n",
    "\n",
    "my_model = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Transformers are a powerful tool for natural language processing.\",\n",
    "    \"Hugging Face provides great pre-trained models for various tasks.\"\n",
    "]\n",
    "\n",
    "# Predictions\n",
    "\n",
    "predictions = my_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Transformers are a powerful tool for natural language processing.\",\n",
    "    \"Hugging Face provides great pre-trained models for various tasks.\"\n",
    "]\n",
    "my_model = pipeline(model=r\"yildizt/results\")\n",
    "my_model(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
