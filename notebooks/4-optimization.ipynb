{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized text: {'text': 'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.', 'labels': 7, 'label_text': 'rec.autos', 'cleaned_text': 'I was wondering if anyone out there could enlighten me on this car I saw the other day It was a door sports car looked to be from the late s early s It was called a Bricklin The doors were really small In addition the front bumper was separate from the rest of the body This is all I know If anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please email', 'input_ids': '[  101  1045  2001  6603  2065  3087  2041  2045  2071  4372  7138  2368\\n  2033  2006  2023  2482  1045  2387  1996  2060  2154  1012  2009  2001\\n  1037  1016  1011  2341  2998  2482  1010  2246  2000  2022  2013  1996\\n  2397 20341  1013  2220 17549  1012  2009  2001  2170  1037  5318  4115\\n  1012  1996  4303  2020  2428  2235  1012  1999  2804  1010  1996  2392\\n 21519  2001  3584  2013  1996  2717  1997  1996  2303  1012  2023  2003\\n  2035  1045  2113  1012  2065  3087  2064  2425  4168  1037  2944  2171\\n  1010  3194 28699  2015  1010  2086  1997  2537  1010  2073  2023  2482\\n  2003  2081  1010  2381  1010  2030  3649 18558  2017  2031  2006  2023\\n 24151  2559  2482  1010  3531  1041  1011  5653  1012   102     0     0\\n     0     0     0     0     0     0     0     0]', 'attention_mask': '[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\\n 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "\n",
    "train_df = pd.read_csv('tokenized_train.csv')\n",
    "test_df = pd.read_csv('tokenized_test.csv')\n",
    "\n",
    "# Convert DataFrames to datasets\n",
    "\n",
    "tokenized_train_dataset = Dataset.from_pandas(train_df)\n",
    "tokenized_test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Print sample tokenized data\n",
    "\n",
    "print(\"Sample tokenized text:\", tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\turab\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373fac2b088a44a7b1da37d74adda7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8406, 'grad_norm': 9.553842544555664, 'learning_rate': 1.529190207156309e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c41f77413e4b938710db00ff89f0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.199527621269226, 'eval_runtime': 22.3077, 'eval_samples_per_second': 337.641, 'eval_steps_per_second': 21.114, 'epoch': 1.0}\n",
      "{'loss': 1.0603, 'grad_norm': 7.716678142547607, 'learning_rate': 1.0583804143126177e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799392fcbc3e4b4d913ead04396f8db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.105939269065857, 'eval_runtime': 26.6128, 'eval_samples_per_second': 283.021, 'eval_steps_per_second': 17.698, 'epoch': 2.0}\n",
      "{'loss': 0.8698, 'grad_norm': 9.898701667785645, 'learning_rate': 5.8757062146892665e-06, 'epoch': 2.12}\n",
      "{'loss': 0.7128, 'grad_norm': 8.077910423278809, 'learning_rate': 1.167608286252354e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6321ed18f5dc430084baf77ff616fdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.078869104385376, 'eval_runtime': 26.6869, 'eval_samples_per_second': 282.236, 'eval_steps_per_second': 17.649, 'epoch': 3.0}\n",
      "{'train_runtime': 427.4488, 'train_samples_per_second': 79.406, 'train_steps_per_second': 4.969, 'train_loss': 1.096146456041354, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2124, training_loss=1.096146456041354, metrics={'train_runtime': 427.4488, 'train_samples_per_second': 79.406, 'train_steps_per_second': 4.969, 'total_flos': 1124412937021440.0, 'train_loss': 1.096146456041354, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "# Reload tokenized datasets from %store\n",
    "\n",
    "%store -r tokenized_train_dataset\n",
    "%store -r tokenized_test_dataset\n",
    "\n",
    "# Define the model\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=20)  # Adjust num_labels as per your dataset\n",
    "\n",
    "# Training arguments (default)\n",
    "\n",
    "training_args_default = TrainingArguments(\n",
    "    output_dir='./results_default',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_default',\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "\n",
    "trainer_default = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_default,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "\n",
    "trainer_default.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\turab\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a466c2ab5c4256a57bd95d2f66b17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c068b737ab4477693da19e4e1367be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1034184694290161, 'eval_runtime': 26.345, 'eval_samples_per_second': 285.899, 'eval_steps_per_second': 8.958, 'epoch': 1.0}\n",
      "{'loss': 0.6438, 'grad_norm': 7.300913333892822, 'learning_rate': 3.587570621468927e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074f36545c304173b773bfef46fb5343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1710067987442017, 'eval_runtime': 31.0844, 'eval_samples_per_second': 242.308, 'eval_steps_per_second': 7.592, 'epoch': 2.0}\n",
      "{'loss': 0.3667, 'grad_norm': 8.049430847167969, 'learning_rate': 2.175141242937853e-05, 'epoch': 2.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4ea795597343df88ea70d5b76f1989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.294228434562683, 'eval_runtime': 33.5971, 'eval_samples_per_second': 224.186, 'eval_steps_per_second': 7.024, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d903dad4cb854bc9b87b9898ffcba360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3508108854293823, 'eval_runtime': 33.3228, 'eval_samples_per_second': 226.031, 'eval_steps_per_second': 7.082, 'epoch': 4.0}\n",
      "{'loss': 0.1874, 'grad_norm': 6.068109512329102, 'learning_rate': 7.627118644067798e-06, 'epoch': 4.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da34a9e4cd8740f3abe048bd56825cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.410793423652649, 'eval_runtime': 33.6701, 'eval_samples_per_second': 223.7, 'eval_steps_per_second': 7.009, 'epoch': 5.0}\n",
      "{'train_runtime': 790.442, 'train_samples_per_second': 71.568, 'train_steps_per_second': 2.239, 'train_loss': 0.35757455556406137, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1770, training_loss=0.35757455556406137, metrics={'train_runtime': 790.442, 'train_samples_per_second': 71.568, 'train_steps_per_second': 2.239, 'total_flos': 1874021561702400.0, 'train_loss': 0.35757455556406137, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized training arguments\n",
    "\n",
    "training_args_optimized = TrainingArguments(\n",
    "    output_dir='./results_optimized',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_optimized',\n",
    ")\n",
    "\n",
    "# Define Trainer for optimized training\n",
    "\n",
    "trainer_optimized = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_optimized,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Start optimized training\n",
    "\n",
    "trainer_optimized.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My defualt training did decent, but the optimized training did better as seen in the metrics such as training loss which was lower. Although the speed was faster for default, the less loss for optimized training alone makes up for and surpasses the default! I used the specific metrics i used because for one, accuarcy is a straightforward indicator of overall preformance. Two, f1 score is good for real world situations where there may be some imbalance with the datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
