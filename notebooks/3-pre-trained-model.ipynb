{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing with the Distilbert base uncased model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14656acd12644efb76f977af1585052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8005a906cdc8416dbd05c3c7ae79e11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized text: {'text': 'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.', 'labels': 7, 'label_text': 'rec.autos', 'cleaned_text': 'I was wondering if anyone out there could enlighten me on this car I saw the other day It was a door sports car looked to be from the late s early s It was called a Bricklin The doors were really small In addition the front bumper was separate from the rest of the body This is all I know If anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please email', 'input_ids': [101, 1045, 2001, 6603, 2065, 3087, 2041, 2045, 2071, 4372, 7138, 2368, 2033, 2006, 2023, 2482, 1045, 2387, 1996, 2060, 2154, 1012, 2009, 2001, 1037, 1016, 1011, 2341, 2998, 2482, 1010, 2246, 2000, 2022, 2013, 1996, 2397, 20341, 1013, 2220, 17549, 1012, 2009, 2001, 2170, 1037, 5318, 4115, 1012, 1996, 4303, 2020, 2428, 2235, 1012, 1999, 2804, 1010, 1996, 2392, 21519, 2001, 3584, 2013, 1996, 2717, 1997, 1996, 2303, 1012, 2023, 2003, 2035, 1045, 2113, 1012, 2065, 3087, 2064, 2425, 4168, 1037, 2944, 2171, 1010, 3194, 28699, 2015, 1010, 2086, 1997, 2537, 1010, 2073, 2023, 2482, 2003, 2081, 1010, 2381, 1010, 2030, 3649, 18558, 2017, 2031, 2006, 2023, 24151, 2559, 2482, 1010, 3531, 1041, 1011, 5653, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming you have loaded your dataset previously\n",
    "%store -r new_ds \n",
    "\n",
    "# Select the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize function with max_length\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets with batch size for efficient memory usage\n",
    "tokenized_train_dataset = new_ds['train'].map(tokenize_function, batched=True, batch_size=1000)\n",
    "tokenized_test_dataset = new_ds['test'].map(tokenize_function, batched=True, batch_size=1000)\n",
    "\n",
    "# Ensure the datasets have the label column correctly mapped\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Print sample tokenized data\n",
    "print(\"Sample tokenized text:\", tokenized_train_dataset[0])\n",
    "\n",
    "# Example of accessing text and labels\n",
    "train_texts = tokenized_train_dataset['text']\n",
    "train_labels = tokenized_train_dataset['labels']\n",
    "\n",
    "test_texts = tokenized_test_dataset['text']\n",
    "test_labels = tokenized_test_dataset['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tokenized_train_dataset' (Dataset)\n",
      "Stored 'tokenized_test_dataset' (Dataset)\n"
     ]
    }
   ],
   "source": [
    "%store tokenized_train_dataset\n",
    "%store tokenized_test_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
